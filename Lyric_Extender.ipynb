{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cde593e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Neil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Neil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74fea6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./archive/spotify_songs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbeb39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code is copy and pasted from Participation Activity #2, \"feedforward_LM_activity\", with slight adjustment in the \n",
    "training data. We also changed the amount of epochs that we used from 10 to 20.\n",
    "'''\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import random\n",
    "\n",
    "# This is a function that prints the number of trainable parameters \n",
    "# of a model.\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# This functions prints all parameters (and their gradients) of a model.\n",
    "def print_parameters(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name)\n",
    "        print(param.data)\n",
    "        print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c97a796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines a bigram language model\n",
    "class FeedforwardLM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_layer = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input):\n",
    "\n",
    "        embedding = self.embedding(input)\n",
    "        hidden_rep = self.relu(self.hidden_layer(embedding))\n",
    "        output = self.output_layer(hidden_rep)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2af1a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, old_vocab=False):\n",
    "    \n",
    "    # Load data, convert text into tensors, construct vocabulary, return data and vocab\n",
    "    if not old_vocab:\n",
    "        vocab = {'<UNK>': 0}\n",
    "    else:\n",
    "        vocab = old_vocab\n",
    "    data = list()\n",
    "    file = open(filename, encoding='utf-8')\n",
    "    for line in file:\n",
    "        line_text = line.split()\n",
    "        line_text = ['<s>'] + line_text + ['</s>']\n",
    "        \n",
    "        if not old_vocab:\n",
    "            # form vocabulary\n",
    "            for word in line_text:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "        \n",
    "        # add words to data\n",
    "        for i, word in enumerate(line_text):\n",
    "            if i < len(line_text) - 1:\n",
    "                if word in vocab:\n",
    "                    idx1 = vocab[word]\n",
    "                else: \n",
    "                    idx1 = vocab['<UNK>']\n",
    "                if line_text[i + 1] in vocab:\n",
    "                    idx2 = vocab[line_text[i + 1]]\n",
    "                else: \n",
    "                    idx2 = vocab['<UNK>']\n",
    "                data.append((torch.tensor(idx1), torch.tensor(idx2)))\n",
    "                \n",
    "    return vocab, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b17fa7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "832\n",
      "832\n",
      "832\n",
      "(tensor(1), tensor(2))\n",
      "(tensor(485), tensor(56))\n"
     ]
    }
   ],
   "source": [
    "# 1) Load the data, and shuffle the training data.\n",
    "vocab, train_data = load_data('train_lyrics.txt')\n",
    "print(len(vocab))\n",
    "_, dev_data = load_data('dev_lyrics.txt', vocab)\n",
    "print(len(vocab))\n",
    "_, test_data = load_data('test_lyrics.txt', vocab)\n",
    "print(len(vocab))\n",
    "\n",
    "print(train_data[0])\n",
    "random.shuffle(train_data)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7c7f3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21797"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Initialize our model.\n",
    "\n",
    "our_lm = FeedforwardLM(len(vocab), 10, 15)\n",
    "count_parameters(our_lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "858d00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Implement validation function\n",
    "    av_loss = 0\n",
    "    for (x, y) in data[:1000]:\n",
    "        \n",
    "        # a) calculate probs / get an output\n",
    "        y_raw = model(x)\n",
    "        \n",
    "        # b) compute loss\n",
    "        loss = ce(y_raw.unsqueeze(0),y.unsqueeze(0))\n",
    "        av_loss += loss\n",
    "\n",
    "    av_loss = av_loss/len(data[:1000])\n",
    "    \n",
    "    print(\"Average loss: \" + str(av_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab3f1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Epoch: 1 ###\n",
      "Average loss: tensor(6.6326, grad_fn=<DivBackward0>)\n",
      "tensor(4.7961, grad_fn=<DivBackward0>)\n",
      "### Epoch: 2 ###\n",
      "Average loss: tensor(6.9438, grad_fn=<DivBackward0>)\n",
      "tensor(4.3230, grad_fn=<DivBackward0>)\n",
      "### Epoch: 3 ###\n",
      "Average loss: tensor(7.2048, grad_fn=<DivBackward0>)\n",
      "tensor(3.9856, grad_fn=<DivBackward0>)\n",
      "### Epoch: 4 ###\n",
      "Average loss: tensor(7.5044, grad_fn=<DivBackward0>)\n",
      "tensor(3.7286, grad_fn=<DivBackward0>)\n",
      "### Epoch: 5 ###\n",
      "Average loss: tensor(7.7607, grad_fn=<DivBackward0>)\n",
      "tensor(3.5551, grad_fn=<DivBackward0>)\n",
      "### Epoch: 6 ###\n",
      "Average loss: tensor(7.9974, grad_fn=<DivBackward0>)\n",
      "tensor(3.4105, grad_fn=<DivBackward0>)\n",
      "### Epoch: 7 ###\n",
      "Average loss: tensor(8.4696, grad_fn=<DivBackward0>)\n",
      "tensor(3.3055, grad_fn=<DivBackward0>)\n",
      "### Epoch: 8 ###\n",
      "Average loss: tensor(8.4718, grad_fn=<DivBackward0>)\n",
      "tensor(3.2082, grad_fn=<DivBackward0>)\n",
      "### Epoch: 9 ###\n",
      "Average loss: tensor(8.7441, grad_fn=<DivBackward0>)\n",
      "tensor(3.1281, grad_fn=<DivBackward0>)\n",
      "### Epoch: 10 ###\n",
      "Average loss: tensor(8.6494, grad_fn=<DivBackward0>)\n",
      "tensor(3.0913, grad_fn=<DivBackward0>)\n",
      "### Epoch: 11 ###\n",
      "Average loss: tensor(8.8212, grad_fn=<DivBackward0>)\n",
      "tensor(3.0370, grad_fn=<DivBackward0>)\n",
      "### Epoch: 12 ###\n",
      "Average loss: tensor(9.3931, grad_fn=<DivBackward0>)\n",
      "tensor(2.9668, grad_fn=<DivBackward0>)\n",
      "### Epoch: 13 ###\n",
      "Average loss: tensor(9.5409, grad_fn=<DivBackward0>)\n",
      "tensor(2.9405, grad_fn=<DivBackward0>)\n",
      "### Epoch: 14 ###\n",
      "Average loss: tensor(9.7889, grad_fn=<DivBackward0>)\n",
      "tensor(2.9152, grad_fn=<DivBackward0>)\n",
      "### Epoch: 15 ###\n",
      "Average loss: tensor(9.3765, grad_fn=<DivBackward0>)\n",
      "tensor(2.8451, grad_fn=<DivBackward0>)\n",
      "### Epoch: 16 ###\n",
      "Average loss: tensor(9.4243, grad_fn=<DivBackward0>)\n",
      "tensor(2.8052, grad_fn=<DivBackward0>)\n",
      "### Epoch: 17 ###\n",
      "Average loss: tensor(9.6774, grad_fn=<DivBackward0>)\n",
      "tensor(2.8025, grad_fn=<DivBackward0>)\n",
      "### Epoch: 18 ###\n",
      "Average loss: tensor(9.9161, grad_fn=<DivBackward0>)\n",
      "tensor(2.7900, grad_fn=<DivBackward0>)\n",
      "### Epoch: 19 ###\n",
      "Average loss: tensor(10.0867, grad_fn=<DivBackward0>)\n",
      "tensor(2.7337, grad_fn=<DivBackward0>)\n",
      "### Epoch: 20 ###\n",
      "Average loss: tensor(10.3340, grad_fn=<DivBackward0>)\n",
      "tensor(2.8096, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 3) Now we train our model.\n",
    "\n",
    "epochs = 20\n",
    "ce = nn.CrossEntropyLoss()\n",
    "softmax = nn.Softmax(dim=0)\n",
    "optimizer = optim.SGD(our_lm.parameters(), lr=0.1)\n",
    "\n",
    "for i in range(epochs):\n",
    "    print('### Epoch: ' + str(i+1) + ' ###')\n",
    "    av_loss = 0\n",
    "    our_lm.train()\n",
    "    for (x, y) in train_data[:10000]:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # a) calculate probs / get an output\n",
    "        y_raw = our_lm(x)\n",
    "        y_hat = softmax(y_raw)\n",
    "        \n",
    "        # b) compute loss\n",
    "        loss = ce(y_raw.unsqueeze(0),y.unsqueeze(0))\n",
    "        av_loss += loss\n",
    "        \n",
    "        # c) get the gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # d) update the weights\n",
    "        optimizer.step()\n",
    "    validate(our_lm, dev_data)\n",
    "    print(av_loss/len(train_data[:10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb88da1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next word: know\n",
      "know\n"
     ]
    }
   ],
   "source": [
    "# 1) Choose a starting word/words\n",
    "\n",
    "starting_words = [\"We're\"]\n",
    "def pred_word(starting_words):\n",
    "    # 2) Find the index of the word, create a tensor, and pass it through the LM\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor([vocab[word] for word in starting_words]).unsqueeze(0)  # Convert starting words to tensor\n",
    "        output_tensor = our_lm(input_tensor)  # Pass the tensor through the language model\n",
    "\n",
    "    # 3) Find the predicted next word from the LM's output\n",
    "    predicted_next_word_index = torch.argmax(output_tensor).item()\n",
    "\n",
    "    # 4) Decode the next word and print it\n",
    "    for word, index in vocab.items():\n",
    "        if index == predicted_next_word_index:\n",
    "            print(\"Predicted next word:\", word)\n",
    "            break\n",
    "\n",
    "    return word\n",
    "x = pred_word(starting_words)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d1ffe",
   "metadata": {},
   "source": [
    "#Makes a sentence using the most common word after the inputed word for an input of x \"sentence_len\" amount of words, which in our case is 10\n",
    "def Make_Sentence(starting_words, sentence_len, sentence, curr_len = 0):\n",
    "    #Base case of recursive function\n",
    "    if curr_len == sentence_len:\n",
    "        return sentence\n",
    "    #predictes the next word based off the previous word\n",
    "    starting_words_next = pred_word(starting_words)\n",
    "    sentence.append(starting_words_next)\n",
    "    Make_Sentence([starting_words_next], sentence_len, sentence, curr_len +1)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "580ab5eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next word: for\n",
      "Predicted next word: ya\n",
      "Predicted next word: That\n",
      "Predicted next word: morning\n",
      "Predicted next word: coffee,\n",
      "Predicted next word: brewed\n",
      "Predicted next word: it\n",
      "Predicted next word: for\n",
      "Predicted next word: ya\n",
      "Predicted next word: That\n",
      "['baby', 'for', 'ya', 'That', 'morning', 'coffee,', 'brewed', 'it', 'for', 'ya', 'That']\n"
     ]
    }
   ],
   "source": [
    "x = Make_Sentence([\"baby\"], 10, [\"baby\"])\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e074774",
   "metadata": {},
   "source": [
    "In reality, we used 4 different words for our song that we used in the presentation \"I\", \"oh\", \"baby\", and \"back\". However, since we reran the code since then with a different training and test set, we cannot reproduce this result. But the final result was as follows:\n",
    "\n",
    "['I', \"can't\", 'prove', 'it', 'haunts', 'her', 'I', 'guess', 'on', 'Come', 'on']\n",
    "\n",
    "['oh', 'Switch', 'it', 'haunts', 'her', 'I', 'guess', 'on', 'Come', 'on', 'Come']\n",
    "\n",
    "['baby', 'You', 'can', 'You', 'can', 'You', 'can', 'You', 'can', 'You', 'can']\n",
    "\n",
    "['back', 'of', 'you', 'for', 'ya', 'I', 'guess', 'on', 'Come', 'on', 'Come']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
